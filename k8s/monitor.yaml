apiVersion: v1
kind: Secret
metadata:
  name: s3-log-keys
  namespace: ${KUBE_NAMESPACE}
type: Opaque
data:
  AWS_ACCESS_KEY_ID: ${SCW_LOG_ACCESS_KEY_B64}
  AWS_SECRET_ACCESS_KEY: ${SCW_LOG_SECRET_KEY_B64}
---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: monitor
  namespace: monitor
spec:
  version: ${ELASTIC_VERSION}
  volumeClaimDeletePolicy: ${ELASTIC_STORAGE_POLICY}
  nodeSets:
  - name: default
    count: ${ELASTIC_NODES}
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: ${ELASTIC_STORAGE_SIZE}
    podTemplate:
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        containers:
        - name: elasticsearch
          env:
          - name: ES_JAVA_OPTS
            value: -Xms${ELASTIC_MEM_JVM} -Xmx${ELASTIC_MEM_JVM}
          resources:
            requests:
              memory: ${ELASTIC_MEM}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-configmap
  namespace: monitor
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
  logstash.conf: |
    input {
      file {
        path => "/var/log/s3/**/*.jsonl"
        mode => "read"
        file_completed_action => "log"
        file_completed_log_path => "/usr/share/logstash/data/plugins/inputs/file/completed.log"
        discover_interval => 60
        stat_interval => 1
        codec => "json"
      }
      # s3 {
      #   access_key_id => "${AWS_ACCESS_KEY_ID}"
      #   secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
      #   bucket => "${SCW_LOG_BUCKET}"
      #   region => "${SCW_REGION}"
      #   endpoint => "https://s3.${SCW_REGION}.scw.cloud"
      #   codec => "json_lines"
      #   additional_settings => {
      #     force_path_style => true
      #     ssl_verify_peer => false
      #     http_wire_trace => true
      #   }
      # }
    }
    filter {
      date {
        match => [ "date" , "ISO8601" ]
      }
      mutate {
        add_field => {
          "source" => "%{[path]}"
        }
      }
      grok {
        match => {
          "[source]" => ".*/(?<kubernetes_cluster_name>judilibre-scw-[^\/]*)-(master|dev)/.*$"
        }
        tag_on_failure => [ "kubernetes_cluster_name_error" ]
      }
      mutate {
        rename => {
          "[kubernetes_labels][app]" => "[kubernetes_labels][app.kubernetes.io/name]"
        }
      }
      if [cpu_p] or [Mem.total] or [write_size] {
        mutate {
          add_field => { "log_type" => "metrics" }
        }
      } else {
        if [kubernetes_container_name] == "nginx-ingress-controller" or [kubernetes_namespace_name] =~ /^judilibre/ {
          grok {
            match => {
              "log" => "%{TIMESTAMP_ISO8601:container_timestamp} %{WORD:container_output} %{WORD:container_output_mode} %{GREEDYDATA:container_log}"
            }
            tag_on_failure => [ "kubernetes_log_parse_error" ]
          }
          if [container_output] {
              mutate {
                remove_field => ["log"]
              }
            if [container_output] == "stderr" {
              mutate {
                add_tag => ["container_error"]
              }
            }
            if [kubernetes_container_name] == "nginx-ingress-controller" {
              if [container_output] != "stderr" {
                mutate {
                  add_field => { "log_type" => "web_access" }
                }
                grok {
                  match => { "container_log" => "%{IPORHOST:clientip} (?:-|(%{WORD}.%{WORD})) (-|%{USER:http_user}) \[%{HTTPDATE:timestamp_request}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" (-|%{NUMBER:status;long}) (-|%{NUMBER:body_bytes_sent;long}) (?:\"(?:%{URI:http_referrer}|-)\") \"%{DATA:http_user_agent}\" (-|%{NUMBER:request_length;long}) (-|%{NUMBER:request_time;double}) \[(?:%{DATA:proxy_upstream_name})\] \[(?:%{DATA:proxy_alternative_upstream_name})\] (-|%{NOTSPACE:upstream_addr}|(?<upstream_addr>(\d+.\d+.\d+.\d+:\d+(, )?|-)+)) (-|%{NUMBER:upstream_response_length;long}|(?<upstream_response_length>(\d+(, )?|-)+)) (-|%{NUMBER:upstream_response_time;double}|(?<upstream_response_time>([0-9\.]+(, )?|-)+)) (-|%{NUMBER:upstream_status;long}|(?<upstream_status>(\d+(, )?|-)+)) %{NOTSPACE:request_id}" }
                }
                if "_grokparsefailure" in [tags] {
                  mutate {
                    add_tag => [ "nginx_log_parse_error" ]
                  }
                } else {
                  mutate {
                    remove_field => ["container_log"]
                    convert => {
                      body_bytes_sent => integer
                      request_length => integer
                      request_time => float
                    }
                  }
                  grok {
                    match => [ "request", "%{URIPARAM:request_params}" ]
                  }
                  if [request_params] {
                    mutate {
                      gsub => [ "request_params", "(\?|\[\]|\/)", "" ]
                      gsub => [ "request_params", "\+", " " ]
                      lowercase => [ "request_params" ]
                    }
                    urldecode {
                      field => "request_params"
                    }
                    kv {
                      prefix => "request_params_"
                      source => "request_params"
                      field_split => "&"
                    }
                  }
                  geoip {
                    source => "clientip"
                  }
                  if [proxy_upstream_name] =~ /search/ {
                    grok {
                      match => {
                        "[request]" => "^/(?<request_api>(decision|search|taxonomy|stats|healthcheck)).*"
                      }
                      tag_on_failure => [ "request_invalid" ]
                    }
                  } else if [proxy_upstream_name] =~ /admin/ {
                    grok {
                      match => {
                        "[request]" => "^/(?<request_api>(admin|delete|import|index)).*"
                      }
                      tag_on_failure => [ "request_invalid" ]
                    }
                  }
                }
              } else {
                mutate {
                  add_field => { "log_type" => "web_error" }
                }
              }
            } else {
              if [kubernetes_container_name] == "nginx-ingress-controller" {
                mutate {
                  add_field => { "log_type" => "web_error" }
                }
              } else {
                mutate {
                  add_field => { "log_type" => "judilibre" }
                }
              }
            }
          } else {
            mutate {
              add_field => { "log_type" => "judilibre" }
            }
          }
        } else {
          drop {}
          # if [kubernetes_container_name] {
          #   mutate {
          #     add_field => { "log_type" => "kubernetes" }
          #   }
          # } else {
          #   mutate {
          #     add_field => { "log_type" => "other" }
          #   }
          # }
        }
      }
    }
    output {
      elasticsearch {
        index => "logstash-%{[kubernetes_cluster_name]}-%{[log_type]}-%{+YYYY.MM}"
        user => "elastic"
        password => "${ELASTIC_PASSWORD}"
        hosts => ["https://monitor-es-http:9200"]
        cacert => "/etc/logstash/certificates/ca.crt"
        ssl => true
        action => "create"
      }
    }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-log-mirror
  namespace: monitor
  labels:
    app: logstash
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 75Gi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: s3-rclone-sync
  namespace: monitor
spec:
  schedule: "* * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: 3600
      template:
        spec:
          containers:
          - name: rclone
            image: rclone/rclone:latest
            volumeMounts:
              - name: s3-log-mirror
                mountPath: /var/log/s3
            command:
            - /bin/sh
            args:
            - -c
            - |
              rclone -v copy --ignore-existing s3:${SCW_LOG_BUCKET} /var/log/s3
            env:
            - name: RCLONE_CONFIG_S3_TYPE
              value: s3
            - name: RCLONE_CONFIG_S3_ENV_AUTH
              value: "false"
            - name: RCLONE_CONFIG_S3_ENDPOINT
              value: s3.${SCW_REGION}.scw.cloud
            - name: RCLONE_CONFIG_S3_REGION
              value: ${SCW_REGION}
            - name: RCLONE_CONFIG_S3_SERVER_SIDE_ENCRYPTION
              value: ""
            - name: RCLONE_CONFIG_S3_FORCE_PATH_STYLE
              value: "false"
            - name: RCLONE_CONFIG_S3_LOCATION_CONSTRAINT
              value: ""
            - name: RCLONE_CONFIG_S3_STORAGE_CLASS
              value: ""
            - name: RCLONE_CONFIG_S3_ACL
              value: private
            - name: RCLONE_CONFIG_S3_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-log-keys
                  key: AWS_ACCESS_KEY_ID
            - name: RCLONE_CONFIG_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-log-keys
                  key: AWS_SECRET_ACCESS_KEY
          restartPolicy: Never
          volumes:
          - name: s3-log-mirror
            persistentVolumeClaim:
              claimName: s3-log-mirror
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash-deployment
  namespace: monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      initContainers:
      - name: install-plugins
        image: docker.elastic.co/logstash/logstash:${ELASTIC_VERSION}
        command:
        - sh
        - -c
        - |
          bin/logstash-plugin install logstash-codec-json_lines logstash-filter-urldecode
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:${ELASTIC_VERSION}
        ports:
        - containerPort: 5044
        env:
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: ${APP_GROUP}-es-elastic-user
                key: elastic
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-log-keys
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-log-keys
                key: AWS_SECRET_ACCESS_KEY
        volumeMounts:
          - name: config-volume
            mountPath: /usr/share/logstash/config
          - name: logstash-pipeline-volume
            mountPath: /usr/share/logstash/pipeline
          - name: cert-ca
            mountPath: /etc/logstash/certificates
            readOnly: true
          - name: s3-log-mirror
            readOnly: true
            mountPath: /var/log/s3/
      volumes:
      - name: config-volume
        configMap:
          name: logstash-configmap
          items:
            - key: logstash.yml
              path: logstash.yml
      - name: logstash-pipeline-volume
        configMap:
          name: logstash-configmap
          items:
            - key: logstash.conf
              path: logstash.conf
      - name: cert-ca
        secret:
          secretName: monitor-es-http-certs-public
      - name: s3-log-mirror
        persistentVolumeClaim:
          claimName: s3-log-mirror
# ---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: filebeat-config
#   namespace: monitor
#   labels:
#     app: filebeat
# data:
#   filebeat.yml: |-
#     filebeat.inputs:
#     - type: aws-s3
#       bucket_arn: "arn:scw.cloud:s3:fr-par::${SCW_LOG_BUCKET}"
#       bucket_regional_domain_name: fr-par
#       endpoint: scw.cloud
#       number_of_workers: 5
#       bucket_list_interval: 60s
#       expand_event_list_from_field: Records
#       json.keys_under_root: true
#       include_s3_metadata:
#         - key

#     output.elasticsearch:
#       hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}']
#       username: ${ELASTICSEARCH_USERNAME}
#       password: ${ELASTICSEARCH_PASSWORD}
# ---
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: filebeat-data
#   namespace: monitor
#   labels:
#     app: filebeat
# spec:
#   accessModes:
#   - ReadWriteOnce
#   resources:
#     requests:
#       storage: 100Gi
# ---
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: filebeat-deployment
#   namespace: monitor
#   labels:
#     app: filebeat
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       app: filebeat
#   template:
#     metadata:
#       labels:
#         app: filebeat
#     spec:
#       terminationGracePeriodSeconds: 30
#       initContainers:
#       # By default k8s mounts the data directory as root, which renders it inaccessible to Elasticsearch.
#       - name: fix-permissions
#         image: busybox
#         command: ["sh", "-c", "chown -R 1000:1000 /usr/share/filebeat/data"]
#         securityContext:
#           privileged: true
#         volumeMounts:
#         - name: data
#           mountPath: /usr/share/filebeat/data
#       containers:
#       - name: filebeat
#         image: docker.elastic.co/beats/filebeat:7.15.0
#         args: [
#           "-c", "/etc/filebeat.yml",
#           "-e",
#         ]
#         env:
#         - name: ELASTICSEARCH_HOST
#           value: monitor-es-http
#         - name: ELASTICSEARCH_PORT
#           value: "9200"
#         - name: ELASTICSEARCH_USERNAME
#           value: elastic
#         - name: ELASTICSEARCH_PASSWORD
#           valueFrom:
#             secretKeyRef:
#               name: ${APP_GROUP}-es-elastic-user
#               key: elastic
#         - name: AWS_ACCESS_KEY_ID
#           valueFrom:
#             secretKeyRef:
#               name: s3-log-keys
#               key: AWS_ACCESS_KEY_ID
#         - name: AWS_SECRET_ACCESS_KEY
#           valueFrom:
#             secretKeyRef:
#               name: s3-log-keys
#               key: AWS_SECRET_ACCESS_KEY
#         volumeMounts:
#         - name: config
#           mountPath: /etc/filebeat.yml
#           readOnly: true
#           subPath: filebeat.yml
#         - name: data
#           mountPath: /usr/share/filebeat/data
#       volumes:
#       - name: config
#         configMap:
#           defaultMode: 0640
#           name: filebeat-config
#       # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart
#       - name: data
#         persistentVolumeClaim:
#           claimName: filebeat-
# ---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: vector
#   namespace: monitor
#   labels:
#     app: vector
# data:
#   # We leave `vector.toml` file name available to let externally managed config
#   # maps to provide it.
#   vector.toml: |
#     # Configuration for vector.
#     # Docs: https://vector.dev/docs/

#     data_dir = "/vector-data-dir"

#     [api]
#       enabled = false

#     [log_schema]
#       host_key = "host"
#       message_key = "message"
#       source_type_key = "source_type"
#       timestamp_key = "timestamp"

#     # Accept logs from Vector agents.
#     [sources.scw_logs]
#       type = "aws_s3"
#       endpoint = "https://s3.fr-par.scw.cloud"
#       bucket = "${SCW_LOG_BUCKET}"

# ---
# apiVersion: v1
# kind: ServiceAccount
# metadata:
#   name: vector
#   namespace: monitor
#   labels:
#     app.kubernetes.io/name: vector
#     app.kubernetes.io/instance: vector
# automountServiceAccountToken: true
# ---
# apiVersion: apps/v1
# kind: StatefulSet
# metadata:
#   name: vector
#   namespace: monitor
#   labels:
#     app.kubernetes.io/name: vector
#     app.kubernetes.io/instance: vector
# spec:
#   serviceName: vector
#   selector:
#     matchLabels:
#       app.kubernetes.io/name: vector
#       app.kubernetes.io/instance: vector
#       app.kubernetes.io/component: aggregator
#   podManagementPolicy: "Parallel"
#   replicas: 1
#   template:
#     metadata:
#       labels:
#         app.kubernetes.io/name: vector
#         app.kubernetes.io/instance: vector
#         app.kubernetes.io/component: aggregator
#         vector.dev/exclude: "true"
#     spec:
#       containers:
#         - name: vector
#           image: "timberio/vector:latest-debian"
#           args:
#             - --config-dir
#             - /etc/vector/
#           env:
#             - name: ELASTIC_PASSWORD
#               valueFrom:
#                 secretKeyRef:
#                   name: ${APP_GROUP}-es-elastic-user
#                   key: elastic
#             - name: AWS_ACCESS_KEY_ID
#               valueFrom:
#                 secretKeyRef:
#                   name: s3-log-keys
#                   key: AWS_ACCESS_KEY_ID
#             - name: AWS_SECRET_ACCESS_KEY
#               valueFrom:
#                 secretKeyRef:
#                   name: s3-log-keys
#                   key: AWS_SECRET_ACCESS_KEY
#             - name: LOG
#               value: info
#           volumeMounts:
#             # Vector data dir mount.
#             - name: vector-data
#               mountPath: "/vector-data-dir"
#             # Vector config dir mount.
#             - name: config-dir
#               mountPath: /etc/vector
#               readOnly: true
#             # Extra volumes.
#       terminationGracePeriodSeconds: 60
#       tolerations:
#         - effect: NoSchedule
#           key: node-role.kubernetes.io/master
#       volumes:
#         # Vector config dir.
#         - name: config-dir
#           projected:
#             sources:
#               - configMap:
#                   name: vector
#   volumeClaimTemplates:
#     - metadata:
#         name: vector-data
#       spec:
#         accessModes:
#         - ReadWriteOnce
#         resources:
#           requests:
#             storage: 1Gi
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: monitor
  namespace: monitor
spec:
  version: ${ELASTIC_VERSION}
  count: 1
  elasticsearchRef:
    name: monitor
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${APP_ID}-route
  namespace: ${KUBE_NAMESPACE}
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/whitelist-source-range: "${IP_WHITELIST}"
    nginx.ingress.kubernetes.io/server-snippet: |
      error_page 403 @forbidden;
      location @forbidden {
        return 301 https://www.courdecassation.fr;
      }
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - ${APP_HOST}
    secretName: ${APP_ID}-cert-${ACME}
  rules:
  - host: ${APP_HOST}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: monitor-kb-http
            port:
              number: 5601

