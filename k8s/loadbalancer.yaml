apiVersion: v1
kind: Service
metadata:
  name: ${APP_ID}-loadbalancer
  namespace: kube-system
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  annotations:
    service.beta.kubernetes.io/scw-loadbalancer-proxy-protocol-v2: "*"
    service.beta.kubernetes.io/scw-loadbalancer-zone: ${SCW_ZONE}
    service.beta.kubernetes.io/scw-loadbalancer-force-internal-ip: "true"
spec:
  type: LoadBalancer
  ${APP_RESERVED_IP_SPEC}
  ports:
  - port: 80
    name: http
    targetPort: 80
  - port: 443
    name: https
    targetPort: 443
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
# use proxy-protocol
apiVersion: v1
kind: ConfigMap
metadata:
  name: ingress-nginx-configuration
  namespace: kube-system
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
data:
  use-proxy-protocol: "true"
  no-tls-redirect-locations: "/.well-known/acme-challenge"
  #enable-opentracing: "true"
  #enable-modsecurity: "true"
  #enable-owasp-core-rules: "true"
---
# workaround https://github.com/compumike/hairpin-proxy
# for bug https://github.com/jetstack/cert-manager/issues/466 in http01 solver
# dns01 could be used but scaleway DNS tokens aren't scoped enough for secured Kube usage
apiVersion: v1
kind: Namespace
metadata:
  name: hairpin-proxy

---

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hairpin-proxy-haproxy
  name: hairpin-proxy-haproxy
  namespace: hairpin-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hairpin-proxy-haproxy
  template:
    metadata:
      labels:
        app: hairpin-proxy-haproxy
    spec:
      containers:
        - image: compumike/hairpin-proxy-haproxy:0.2.1
          name: main
          resources:
            requests:
              memory: "100Mi"
              cpu: "10m"
            limits:
              memory: "200Mi"
              cpu: "50m"
          env:
            - name: TARGET_SERVER
              value: ${APP_ID}-loadbalancer.kube-system.svc.cluster.local

---

apiVersion: v1
kind: Service
metadata:
  name: hairpin-proxy
  namespace: hairpin-proxy
spec:
  selector:
    app: hairpin-proxy-haproxy
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443

---

kind: ServiceAccount
apiVersion: v1
metadata:
  name: hairpin-proxy-controller-sa
  namespace: hairpin-proxy

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hairpin-proxy-controller-cr
rules:
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: hairpin-proxy-controller-crb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: hairpin-proxy-controller-cr
subjects:
  - kind: ServiceAccount
    name: hairpin-proxy-controller-sa
    namespace: hairpin-proxy

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: hairpin-proxy-controller-r
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources:
      - configmaps
    resourceNames:
      - coredns
    verbs:
      - get
      - watch
      - update

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: hairpin-proxy-controller-rb
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hairpin-proxy-controller-r
subjects:
  - kind: ServiceAccount
    name: hairpin-proxy-controller-sa
    namespace: hairpin-proxy

---

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hairpin-proxy-controller
  name: hairpin-proxy-controller
  namespace: hairpin-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hairpin-proxy-controller
  template:
    metadata:
      labels:
        app: hairpin-proxy-controller
    spec:
      serviceAccountName: hairpin-proxy-controller-sa
      securityContext:
        runAsUser: 405
        runAsGroup: 65533
      containers:
        - image: compumike/hairpin-proxy-controller:0.2.1
          name: main
          resources:
            requests:
              memory: "50Mi"
              cpu: "10m"
            limits:
              memory: "100Mi"
              cpu: "50m"